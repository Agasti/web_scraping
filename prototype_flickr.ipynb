{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, textwrap, time, json, re, requests, random, datetime, copy\n",
    "from datetime import datetime as dt\n",
    "from copy import deepcopy\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "global driver, params, AFTER_DATE ,BEFORE_DATE ,COURTESY_SLEEP ,PHOTOS_PER_PAGE ,VERBOSE ,TEST ,DUMP_PATH ,ADD_EXTRAS, HEADLESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFTER_DATE = dt.fromtimestamp(1570579058.0)\n",
    "BEFORE_DATE = dt.fromtimestamp(1577592826.384834)\n",
    "PHOTOS_PER_PAGE =  500\n",
    "VERBOSE =  4\n",
    "DUMP_PATH = './'\n",
    "TEST = True\n",
    "ADD_EXTRAS = ' url_o,original_format,date_taken,date_upload,geo'\n",
    "HEADLESS = True\n",
    "COURTESY_SLEEP = [0, 0.000000001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=textwrap.dedent('''\\\n",
    "        scrape JSONs containing photos from flickr\n",
    "         '''))\n",
    "\n",
    "parser.add_argument(\"-a\", \"--after_date\", help=\"Start date (in unix timestamp format). Defaults to yesterday\", type=float, default=dt.now().timestamp() - (30 * 24 * 3600))\n",
    "parser.add_argument(\"-b\", \"--before_date\", help=\"End date (in unix timestamp format). Defaults to now\", type=float, default=dt.now().timestamp())\n",
    "parser.add_argument(\"-s\", \"--courtesy_sleep\", help=\"Range (in string format) from which a random value will be chosen to sleep randomly. example: '1.3, 2.7'\", type=str, default=\"1.3, 2.7\")\n",
    "parser.add_argument(\"-n\", \"--photos_per_page\", help=\"Photos per file. Default is 500 which is the maximum\", type=int, default=500)\n",
    "parser.add_argument(\"-v\", \"--verbose\", help=\"increase output verbosity\", action=\"count\", default=0)\n",
    "parser.add_argument(\"-p\", \"--dump_path\", help=\"Path where to dump json files\", type=str, default='./')\n",
    "parser.add_argument(\"-t\", \"--test\", help=\"test mode\", action='store_true')\n",
    "parser.add_argument(\"-w\", \"--webdriver\", help=\"Turn off headless mode for on the chrome webdriver\", action='store_false')\n",
    "parser.add_argument(\"-x\", \"--add_extras\", help=\"extra json fields to request. Defaults to 'url_o,original_format,date_taken,date_upload,geo'\", type=str, default=\"url_o,original_format,date_taken,date_upload,geo\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "args.__dict__['HEADLESS'] = args.webdriver\n",
    "\n",
    "# ansigning global variables from cmdline args for easy typing\n",
    "for each in args.__dict__: globals()[each.upper()] = args.__dict__[each]\n",
    "\n",
    "# making sure the dates are in datetime format\n",
    "for each in [\"AFTER_DATE\", \"BEFORE_DATE\"]:\n",
    "    if type(each) != type(dt.now()):\n",
    "        try:\n",
    "            globals()[each] = dt.fromtimestamp(globals()[each])\n",
    "        except Exception as e:\n",
    "            print(f\"please make sure the dates entered are in unix timestamps format: {e}\")\n",
    "\n",
    "# printing parameters for easy debugging\n",
    "if VERBOSE >=3:\n",
    "    print(\"\".ljust(120, \"_\") + \"\\nscript parameters\")\n",
    "    for each in args.__dict__:\n",
    "        print(f\"{each.upper()}: {args.__dict__[each]}\")\n",
    "    print(\"\".ljust(120, \"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#COURTESY_SLEEP = [float(COURTESY_SLEEP.split(',')[0]) , float(COURTESY_SLEEP.split(',')[1])]\n",
    "#if TEST and VERBOSE > 3: COURTESY_SLEEP = [0, 0.000000001]\n",
    "\n",
    "\n",
    "DATA_PATH = ('./test/' if TEST else DUMP_PATH)\n",
    "\n",
    "params = {\n",
    "    \"sort\" : \"relevance\",\n",
    "    \"parse_tags\" : \"1\",\n",
    "    \"content_type\" : \"7\",\n",
    "    \"lang\": \"en-US\",\n",
    "    \"has_geo\" :\"1\",\n",
    "    \"media\" : \"photos\",\n",
    "    \"view_all\" : \"1\",\n",
    "    \"text\" : \"clouds\",\n",
    "    \"viewerNSID\": \"\",\n",
    "    \"method\" : \"flickr.photos.search\",\n",
    "    \"csrf\" : \"\",\n",
    "    \"format\" : \"json\",\n",
    "    \"hermes\" : \"1\",\n",
    "    \"hermesClient\" : \"1\",\n",
    "    \"nojsoncallback\" : \"1\",\n",
    "    \"geo_context\": '2', # 0: all , 1: indoors, 2 : outdoors\n",
    "    \"privacy_filter\" : 1\n",
    "}\n",
    "\n",
    "privacy_filters = '''\n",
    "\"public photos\" : '1',\n",
    "\"private photos visible to friends\" : '2',\n",
    "\"private photos visible to family\" : '3',\n",
    "\"private photos visible to friends & family\": '4',\n",
    "\"completely private photos\" : '5'\n",
    "'''\n",
    "\n",
    "FLICKR = 'https://flickr.com/search/'\n",
    "TAGS = ['clouds', 'cloud', 'sky', 'storm', 'weather', 'rain cloud']\n",
    "\n",
    "var_names = [\"api_key\", \"reqId\", \"api_url\", \"extras\"]\n",
    "re_expressions = [r\"(api_key)=([\\dabcdef]*)(&)\", r\"(reqId)=([\\dabcdef]*)(&)\", r\"(https:\\/\\/(\\w+\\.?)+(\\/\\w+)+)(\\?)\", r\"extras=((\\w+(%2)?)+?)?&\"]\n",
    "groups = [2,2,1,1]\n",
    "\n",
    "variables = [dict(zip([\"var_name\", \"regex\", 'group'], each)) for each in [each for each in zip(var_names, re_expressions, groups)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_api_call(call_string):\n",
    "    ''' parsing data from DOM element '''\n",
    "    #creating variables for each parsed data\n",
    "    for each in variables:\n",
    "\n",
    "        if  re.search(each[\"regex\"], string=call_string, flags=re.MULTILINE) != None:\n",
    "            globals()[each[\"var_name\"]] = re.search(each[\"regex\"], string=call_string, flags=re.MULTILINE).group(each[\"group\"])\n",
    "        else:\n",
    "            globals()[each[\"var_name\"]] = None\n",
    "\n",
    "    if VERBOSE >= 2:\n",
    "        print(\"Extracted ajax params:--------\\n\")\n",
    "        for each in var_names:\n",
    "            print(\"%(var)s :     %(value)s\" % {\"var\": each.ljust(10, ' '), \"value\" : globals()[each]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def looping_over_date_range(path, params, session, start, stop, offset):\n",
    "    ''' loops over given date range'''\n",
    "\n",
    "    # copying request parameters to prepare to simultaneous execusion\n",
    "    params_lcl = deepcopy(params)\n",
    "\n",
    "    last_response_time = 0\n",
    "\n",
    "    # initializing time index\n",
    "    params_lcl['min_upload_date'] = stop\n",
    "\n",
    "    while params_lcl['min_upload_date'].timestamp() >= start.timestamp():\n",
    "        if VERBOSE >= 3: print(\"\".ljust(20, '+'))\n",
    "\n",
    "        params_lcl['max_upload_date'] = params_lcl['min_upload_date']\n",
    "        params_lcl['min_upload_date'] -= datetime.timedelta(days=offset)\n",
    "\n",
    "        total_photos = s.get(api_url, params=params_lcl).json()['photos']['total']\n",
    "        if VERBOSE >=3: print(f\"New date range: {params_lcl['min_upload_date']} to {params_lcl['max_upload_date']}______ total photos: {total_photos}\")\n",
    "\n",
    "        params_lcl['min_upload_date'], next_batch_size, offset = find_best_date_range(session=s\n",
    "                                               ,params=params_lcl\n",
    "                                               , start=params_lcl['min_upload_date']\n",
    "                                               , stop=params_lcl['max_upload_date']\n",
    "                                               , total_photos=total_photos\n",
    "                                               , offset=offset)\n",
    "\n",
    "        if VERBOSE >= 2: print(f\"Next suitable range: {params_lcl['min_upload_date']} to {params_lcl['max_upload_date']}______ total photos : {next_batch_size}\".ljust(120, ' '))\n",
    "\n",
    "        if VERBOSE >= 1: print(f'starting JSON dump...')\n",
    "\n",
    "        if TEST and VERBOSE > 3:\n",
    "            print(\" fake writing to file \")\n",
    "        else:\n",
    "            write_each_page_as_json_file(path=DATA_PATH, call_params=params_lcl, session=s)\n",
    "\n",
    "        time.sleep(0.2)\n",
    "        time.sleep(last_response_time * random.uniform(COURTESY_SLEEP[0], COURTESY_SLEEP[1]) * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_date_range(session, params, start, stop, total_photos, offset):\n",
    "\n",
    "    call_params = deepcopy(params)\n",
    "    # creating local variables to avoid multiprocessing issues down the line\n",
    "    call_params['per_page'] = 1\n",
    "    call_params['extras'] = ''\n",
    "\n",
    "    if VERBOSE >=3: print(f\"Finding a better range (in 20 attempts or less) ...\")\n",
    "    repeats = 0\n",
    "\n",
    "    # This loop check whether the returned total photos are just a hair under\n",
    "    # which is the maximum allowed by the api. If not it cleverly adjusts the\n",
    "    # offset. The adjustment value used is difference ratio to the wanted\n",
    "    # number of photos (the assumption for this heuristic is that for a small\n",
    "    # range the uploaded photos density will not change much. And thus the\n",
    "    # difference % when applied to the range will give us a ballpark of the wanted range)\n",
    "\n",
    "    while (int(total_photos) > 4000 or int(total_photos) < 3990) and not (int(total_photos) < 4000 and repeats > 20):\n",
    "\n",
    "        if int(total_photos) > 4000:\n",
    "            if VERBOSE >=3: print(f\"({str(repeats)}): too many    ({total_photos.ljust(5, '+')})\", end = '\\r')\n",
    "            # here the % will be small because we overshot the value wanted\n",
    "            offset = offset * 4000/ int(total_photos)\n",
    "\n",
    "        if int(total_photos) <= 4000:\n",
    "\n",
    "            if VERBOSE >=3: print(f\"({str(repeats)}): not enough  ({total_photos.ljust(5, '-')})\", end = '\\r')\n",
    "            # here the % will be big because we underestimated the range (plus a small nudge)\n",
    "            offset = offset * 4000 / int(total_photos)\n",
    "\n",
    "        start = stop - datetime.timedelta(offset)\n",
    "\n",
    "        # shifting the date range\n",
    "        call_params['min_upload_date'] = start\n",
    "        total_photos = session.get(api_url, params=call_params).json()['photos']['total']\n",
    "\n",
    "        repeats += 1\n",
    "        # if VERBOSE >3 and TEST: time.sleep(1)\n",
    "    return start, total_photos, offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_each_page_as_json_file(path, call_params, session):\n",
    "\n",
    "    for each in added_params: call_params[each] = added_params[each]\n",
    "\n",
    "    # making_sure the request pages are correct\n",
    "    call_params['per_page'] = PHOTOS_PER_PAGE\n",
    "    pages = s.get(api_url, params=call_params).json()['photos']['pages']\n",
    "\n",
    "    for page in range(1, 1 + pages):\n",
    "        print(f\"Requesting page {page}...\".ljust(120, ' '),  end='\\r')\n",
    "\n",
    "        call_params['page'] = page\n",
    "\n",
    "        before = dt.now().timestamp()\n",
    "\n",
    "        try:\n",
    "            response = session.get(api_url, params=call_params)\n",
    "        except Exception as e:\n",
    "            print(f\"Couldn't request JSON data:____ {e}\")\n",
    "        after = dt.now().timestamp()\n",
    "\n",
    "        last_response_time = after - before\n",
    "\n",
    "        # Trying to write JSON data to file\n",
    "        file_to_be_written = f\"{path}{term}_{str(call_params['min_upload_date'].timestamp())}-{str(call_params['max_upload_date'].timestamp())}_{page}.json\"\n",
    "        #if VERBOSE >=3: print(f\" file path to be written: {file_to_be_written}\\n\\n\")\n",
    "        try:\n",
    "            with open(file_to_be_written, 'w') as outfile:\n",
    "                json.dump(response.json(), outfile)\n",
    "\n",
    "            time_it_took = str(round(last_response_time, 2))\n",
    "            print(f\"{file_to_be_written} written succesfully! took {time_it_took} s\")\n",
    "            time.sleep(0.2)\n",
    "        except Exception as e:\n",
    "            print(f\"problem dumping json data: {str(e)}\")\n",
    "        if VERBOSE >=2: print(f'sleeping for {last_response_time * random.uniform(COURTESY_SLEEP[0], COURTESY_SLEEP[1])} seconds... (for courtesy :P )'.ljust(120, ' '), end = '\\r')\n",
    "        time.sleep(last_response_time * random.uniform(COURTESY_SLEEP[0], COURTESY_SLEEP[1]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_call_string():\n",
    "    ''' use selenium to get the api_call string'''\n",
    "    \n",
    "    options = Options()\n",
    "\n",
    "    if HEADLESS: options.add_argument('--headless')\n",
    "\n",
    "    caps = DesiredCapabilities.CHROME\n",
    "    caps['loggingPref'] = {'performance': 'ALL'}\n",
    "\n",
    "    driver = webdriver.Chrome(options = options, desired_capabilities=caps)\n",
    "\n",
    "    xhrCallIntercept_js = \"\"\"\n",
    "    (function(XHR) {\n",
    "      \"use strict\";\n",
    "\n",
    "      var element = document.createElement('div');\n",
    "      element.id = \"interceptedResponse\";\n",
    "      element.appendChild(document.createTextNode(\"\"));\n",
    "      document.body.appendChild(element);\n",
    "\n",
    "      var open = XHR.prototype.open;\n",
    "      var send = XHR.prototype.send;\n",
    "\n",
    "      XHR.prototype.open = function(method, url, async, user, pass) {\n",
    "        this._url = url; // want to track the url requested\n",
    "        open.call(this, method, url, async, user, pass);\n",
    "      };\n",
    "\n",
    "      XHR.prototype.send = function(data) {\n",
    "        var self = this;\n",
    "        var oldOnReadyStateChange;\n",
    "        var url = this._url;\n",
    "\n",
    "        function onReadyStateChange() {\n",
    "          if(self.status === 200 && self.readyState == 4 /* complete */) {\n",
    "            document.getElementById(\"interceptedResponse\").innerHTML +=\n",
    "              '{\"data\":' + self._url + ', \"headers\" :' + self.headers + ' }*****';\n",
    "          }\n",
    "          if(oldOnReadyStateChange) {\n",
    "            oldOnReadyStateChange();\n",
    "          }\n",
    "        }\n",
    "\n",
    "        if(this.addEventListener) {\n",
    "          this.addEventListener(\"readystatechange\", onReadyStateChange,\n",
    "            false);\n",
    "        } else {\n",
    "          oldOnReadyStateChange = this.onreadystatechange;\n",
    "          this.onreadystatechange = onReadyStateChange;\n",
    "        }\n",
    "        send.call(this, data);\n",
    "      }\n",
    "    })(XMLHttpRequest);\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        url = FLICKR + \"?has_geo=1&media=photos&view_all=1&text=\" + TAGS[0]\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        driver.execute_script(xhrCallIntercept_js)\n",
    "\n",
    "        if VERBOSE >=1: print('title : \"{}\"'.format(driver.title))\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error! Cannot open search page: ' + str(e))\n",
    "\n",
    "    wait = False\n",
    "    while wait != True:\n",
    "        if VERBOSE >= 1: print(\"Getting AJAX data...\")\n",
    "        # trying scroll to trigger and api call\n",
    "        try:\n",
    "            if VERBOSE >= 3: print('attempting Scroll!')\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            if VERBOSE >= 3: print('Scrolled...')\n",
    "\n",
    "            # waiting for the api call to be included in the DOM\n",
    "            wait = WebDriverWait(driver, 15).until(EC.text_to_be_present_in_element((By.ID, \"interceptedResponse\"), \"api_key\"))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"intercept failed!:\" + str(e))\n",
    "\n",
    "        intercepts = driver.find_elements_by_id('interceptedResponse')\n",
    "\n",
    "    if wait == True and VERBOSE >= 1: print('ajax call intercepted!\\n')\n",
    "    xhr_api_call = intercepts[0].text\n",
    "\n",
    "    cookies = driver.get_cookies()\n",
    "    driver.close()\n",
    "    \n",
    "    return xhr_api_call, cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    xhr_api_call, cookies = get_api_call_string()\n",
    "\n",
    "    parse_api_call(xhr_api_call)\n",
    "\n",
    "    extras = extras.replace('%2C', ',')\n",
    "\n",
    "    added_params = {\n",
    "        \"extras\" : extras + ','+ ADD_EXTRAS,\n",
    "        \"per_page\" : 1,\n",
    "        \"api_key\" : api_key,\n",
    "        \"reqId\" : reqId,\n",
    "    }\n",
    "\n",
    "    for each in added_params: params[each] = added_params[each]\n",
    "\n",
    "    spoof_webdriver = False\n",
    "\n",
    "    if spoof_webdriver: ua = UserAgent()\n",
    "\n",
    "    with requests.sessions.Session() as s:\n",
    "        for cookie in cookies:\n",
    "            s.cookies.set(cookie['name'], cookie['value'])\n",
    "        if spoof_webdriver: s.headers['User-Agent'] = str(ua.chrome)\n",
    "\n",
    "    offset = 3\n",
    "\n",
    "\n",
    "    TEST_RANGE = TAGS[:-2] if TEST and VERBOSE > 3 else TAGS\n",
    "\n",
    "    for term in TEST_RANGE:\n",
    "\n",
    "        params['text'] = term\n",
    "\n",
    "        first_range = True\n",
    "\n",
    "        start = AFTER_DATE\n",
    "        stop = BEFORE_DATE\n",
    "\n",
    "        params['min_upload_date'] = start\n",
    "        params['max_upload_date'] = stop\n",
    "\n",
    "        looping_over_date_range(path=DATA_PATH, params=params, session=s, start=params['min_upload_date'], stop=params['max_upload_date'], offset = offset)\n",
    "        if VERBOSE >=3:\n",
    "            print(\"\".ljust(120, \"-\"))\n",
    "            print(f\"Finished for term: {term}   in date range : from {start} to {stop}\")\n",
    "            print(\"\".ljust(120, \"-\"))\n",
    "\n",
    "        time.sleep(random.uniform(COURTESY_SLEEP[0], COURTESY_SLEEP[1]) * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
