{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, textwrap, time, json , re, requests ,random, datetime, copy\n",
    "import os.path, csv, shutil\n",
    "from datetime import datetime as dt\n",
    "from tempfile import NamedTemporaryFile\n",
    "from copy import deepcopy\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "global driver, params, AFTER_DATE ,BEFORE_DATE ,COURTESY_SLEEP ,PHOTOS_PER_PAGE\n",
    "global VERBOSE ,TEST ,DUMP_PATH ,ADD_EXTRAS, HEADLESS, GEO_BOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFTER_DATE = dt.fromtimestamp(1546800000) # 2020/01/06\n",
    "#AFTER_DATE = dt.fromtimestamp(1072915200) # 2014/01/01\n",
    "BEFORE_DATE = dt.now()\n",
    "PHOTOS_PER_PAGE =  250\n",
    "VERBOSE = 3\n",
    "DUMP_PATH = './'\n",
    "TEST = True\n",
    "ADD_EXTRAS = 'url_o,original_format,date_taken,date_upload,geo'\n",
    "HEADLESS = True\n",
    "COURTESY_SLEEP = \"0, 0.000000001\"\n",
    "GET_DATES_ONLY = False\n",
    "BBOX_PHOTOS_PER_PAGE = 250\n",
    "GEO_BOX = '-124.799423, 24.750821, -54.517891, 54.306268' # US and Canada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=textwrap.dedent('''\\\n",
    "        scrape JSONs containing photos from flickr\n",
    "         '''))\n",
    "\n",
    "parser.add_argument(\"-a\", \"--after_date\"\n",
    "                    , help=\"Start date (in unix timestamp format).\\Defaults to yesterday\"\n",
    "                    , type=float, default=dt.now().timestamp() - (30 * 24 * 3600))\n",
    "parser.add_argument(\"-b\", \"--before_date\"\n",
    "                    , help=\"End date (in unix timestamp format). Defaults to now\", type=float\n",
    "                    , default=dt.now().timestamp())\n",
    "parser.add_argument(\"-s\", \"--courtesy_sleep\"\n",
    "                    , help=\"Range (in string format) from which a random value will be chosen to sleep randomly. example: '1.3, 2.7'\", type=str, default=\"1.3, 2.7\")\n",
    "parser.add_argument(\"-n\", \"--photos_per_page\"\n",
    "                    , help=\"Photos per file. Default is 500 which is the maximum\", type=int\n",
    "                    , default=500)\n",
    "parser.add_argument(\"-v\", \"--verbose\"\n",
    "                    , help=\"increase output verbosity\", action=\"count\", default=0)\n",
    "parser.add_argument(\"-p\", \"--dump_path\"\n",
    "                    , help=\"Path where to dump json files\", type=str, default='./')\n",
    "parser.add_argument(\"-t\", \"--test\"\n",
    "                    , help=\"test mode\", action='store_true')\n",
    "parser.add_argument(\"-d\", \"--get_dates_only\"\n",
    "                    , help=\"Only scan for suitable ranges and store them on csv file.\\\n",
    "                    \\n suitable ranges are dates where the photos returned aproximates\\\n",
    "                    4000\", action='store_true')\n",
    "parser.add_argument(\"-g\", \"--geo_box\"\n",
    "                    , help=\"Limit the search to the geographical locations in the bounding\\\n",
    "                    box defined by 4 values. The 4 values represent the bottom-left corner \\\n",
    "                    of the box and the top-right corner, minimum_longitude, minimum_latitude, \\\n",
    "                    maximum_longitude, maximum_latitude. Defaults to '-180, -90, 180, 90'\"\n",
    "                    , default='-180, -90, 180, 90')\n",
    "parser.add_argument(\"-w\", \"--webdriver\"\n",
    "                    , help=\"Turn off headless mode for on the chrome webdriver\"\n",
    "                    , action='store_false')\n",
    "parser.add_argument(\"-x\", \"--add_extras\"\n",
    "                    , help=\"extra json fields to request. Defaults to 'url_o,original_format\\\n",
    "                    ,date_taken,date_upload,geo'\", type=str\n",
    "                    , default=\"url_o,original_format,date_taken,date_upload,geo\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "args.__dict__['HEADLESS'] = args.webdriver\n",
    "\n",
    "# ansigning global variables from cmdline args for easy typing\n",
    "for each in args.__dict__: globals()[each.upper()] = args.__dict__[each]\n",
    "\n",
    "# making sure the dates are in datetime format\n",
    "for each in [\"AFTER_DATE\", \"BEFORE_DATE\"]:\n",
    "    if type(each) != type(dt.now()):\n",
    "        try:\n",
    "            globals()[each] = dt.fromtimestamp(globals()[each])\n",
    "        except Exception as e:\n",
    "            print(f\"please make sure the dates entered are in unix timestamps format: {e}\")\n",
    "\n",
    "# printing parameters for easy debugging\n",
    "if VERBOSE >=3:\n",
    "    print(\"\".ljust(120, \"_\") + \"\\nscript parameters\")\n",
    "    for each in args.__dict__:\n",
    "        print(f\"{each.upper()}: {args.__dict__[each]}\")\n",
    "    print(\"\".ljust(120, \"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "    \n",
    "COURTESY_SLEEP = [float(COURTESY_SLEEP.split(',')[0]) , float(COURTESY_SLEEP.split(',')[1])]\n",
    "if TEST and VERBOSE > 3: COURTESY_SLEEP = [0, 0.000000001]\n",
    "\n",
    "\n",
    "DATA_PATH = ('./test/' if TEST else DUMP_PATH)\n",
    "\n",
    "params = {\n",
    "    #\n",
    "    \"bbox\" : GEO_BOX, \n",
    "    \"sort\" : \"relevance\",\n",
    "    \"parse_tags\" : \"1\",\n",
    "    \"content_type\" : \"7\",\n",
    "    \"lang\": \"en-US\",\n",
    "    \"has_geo\" :\"1\",\n",
    "    \"media\" : \"photos\",\n",
    "    \"view_all\" : \"1\",\n",
    "    \"text\" : \"clouds\",\n",
    "    \"viewerNSID\": \"\",\n",
    "    \"method\" : \"flickr.photos.search\",\n",
    "    \"csrf\" : \"\",\n",
    "    \"format\" : \"json\",\n",
    "    \"hermes\" : \"1\",\n",
    "    \"hermesClient\" : \"1\",\n",
    "    \"nojsoncallback\" : \"1\",\n",
    "    \"geo_context\": '2', # 0: all , 1: indoors, 2 : outdoors\n",
    "    \"privacy_filter\" : 1\n",
    "}\n",
    "\n",
    "privacy_filters = '''\n",
    "\"public photos\" : '1',\n",
    "\"private photos visible to friends\" : '2',\n",
    "\"private photos visible to family\" : '3',\n",
    "\"private photos visible to friends & family\": '4',\n",
    "\"completely private photos\" : '5'\n",
    "'''\n",
    "\n",
    "FLICKR = 'https://flickr.com/search/'\n",
    "TAGS = ['rain cloud', 'sun clouds', 'sunny clouds', 'clouds', 'cloud', 'sky', 'storm', 'weather', 'cloudy']\n",
    "\n",
    "var_names = [\"api_key\", \"reqId\", \"api_url\", \"extras\"]\n",
    "re_expressions = [r\"(api_key)=([\\dabcdef]*)(&)\", r\"(reqId)=([\\dabcdef]*)(&)\", r\"(https:\\/\\/(\\w+\\.?)+(\\/\\w+)+)(\\?)\", r\"extras=((\\w+(%2)?)+?)?&\"]\n",
    "groups = [2,2,1,1]\n",
    "\n",
    "variables = [dict(zip([\"var_name\", \"regex\", 'group'], each)) for each in [each for each in zip(var_names, re_expressions, groups)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_api_call_string():\n",
    "    ''' use selenium to get the api_call string'''\n",
    "    \n",
    "    options = Options()\n",
    "\n",
    "    if HEADLESS: options.add_argument('--headless')\n",
    "\n",
    "    caps = DesiredCapabilities.CHROME\n",
    "    caps['loggingPref'] = {'performance': 'ALL'}\n",
    "\n",
    "    driver = webdriver.Chrome(options = options, desired_capabilities=caps)\n",
    "\n",
    "    xhrCallIntercept_js = \"\"\"\n",
    "    (function(XHR) {\n",
    "      \"use strict\";\n",
    "\n",
    "      var element = document.createElement('div');\n",
    "      element.id = \"interceptedResponse\";\n",
    "      element.appendChild(document.createTextNode(\"\"));\n",
    "      document.body.appendChild(element);\n",
    "\n",
    "      var open = XHR.prototype.open;\n",
    "      var send = XHR.prototype.send;\n",
    "\n",
    "      XHR.prototype.open = function(method, url, async, user, pass) {\n",
    "        this._url = url; // want to track the url requested\n",
    "        open.call(this, method, url, async, user, pass);\n",
    "      };\n",
    "\n",
    "      XHR.prototype.send = function(data) {\n",
    "        var self = this;\n",
    "        var oldOnReadyStateChange;\n",
    "        var url = this._url;\n",
    "\n",
    "        function onReadyStateChange() {\n",
    "          if(self.status === 200 && self.readyState == 4 /* complete */) {\n",
    "            document.getElementById(\"interceptedResponse\").innerHTML +=\n",
    "              '{\"data\":' + self._url + ', \"headers\" :' + self.headers + ' }*****';\n",
    "          }\n",
    "          if(oldOnReadyStateChange) {\n",
    "            oldOnReadyStateChange();\n",
    "          }\n",
    "        }\n",
    "\n",
    "        if(this.addEventListener) {\n",
    "          this.addEventListener(\"readystatechange\", onReadyStateChange,\n",
    "            false);\n",
    "        } else {\n",
    "          oldOnReadyStateChange = this.onreadystatechange;\n",
    "          this.onreadystatechange = onReadyStateChange;\n",
    "        }\n",
    "        send.call(this, data);\n",
    "      }\n",
    "    })(XMLHttpRequest);\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        url = FLICKR + \"?has_geo=1&media=photos&view_all=1&text=\" + TAGS[0]\n",
    "\n",
    "        driver.get(url)\n",
    "\n",
    "        driver.execute_script(xhrCallIntercept_js)\n",
    "\n",
    "        if VERBOSE >=1: print('title : \"{}\"'.format(driver.title))\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error! Cannot open search page: ' + str(e))\n",
    "\n",
    "    wait = False\n",
    "    while wait != True:\n",
    "        if VERBOSE >= 1: print(\"Getting AJAX data...\")\n",
    "        # trying scroll to trigger and api call\n",
    "        try:\n",
    "            if VERBOSE >= 3: print('attempting Scroll!')\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            if VERBOSE >= 3: print('Scrolled...')\n",
    "\n",
    "            # waiting for the api call to be included in the DOM\n",
    "            wait = WebDriverWait(driver, 15).until(EC.text_to_be_present_in_element((By.ID, \"interceptedResponse\"), \"api_key\"))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"intercept failed!:\" + str(e))\n",
    "\n",
    "        intercepts = driver.find_elements_by_id('interceptedResponse')\n",
    "\n",
    "    if wait == True and VERBOSE >= 1: print('ajax call intercepted!\\n')\n",
    "    xhr_api_call = intercepts[0].text\n",
    "\n",
    "    cookies = driver.get_cookies()\n",
    "    driver.close()\n",
    "    \n",
    "    return xhr_api_call, cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def parse_api_call(call_string):\n",
    "    ''' parsing data from DOM element '''\n",
    "    #creating variables for each parsed data\n",
    "    for each in variables:\n",
    "\n",
    "        if  re.search(each[\"regex\"], string=call_string, flags=re.MULTILINE) != None:\n",
    "            globals()[each[\"var_name\"]] = re.search(each[\"regex\"], string=call_string, flags=re.MULTILINE).group(each[\"group\"])\n",
    "        else:\n",
    "            globals()[each[\"var_name\"]] = None\n",
    "\n",
    "    if VERBOSE >= 2:\n",
    "        print(\"Extracted ajax params:--------\\n\")\n",
    "        for each in var_names:\n",
    "            print(\"%(var)s :     %(value)s\" % {\"var\": each.ljust(10, ' '), \"value\" : globals()[each]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_date_ranges(path, params, session):\n",
    "   \n",
    "    '''From one big range construct a bunch of contiguous ones joined\n",
    "    end to end containing aproximately 4000 photos each'''\n",
    "    \n",
    "    # copying request parameters to prepare to simultaneous execusion\n",
    "    params_lcl = deepcopy(params)\n",
    "\n",
    "    TERMS = TAGS[-2:] if TEST and VERBOSE > 3 else TAGS\n",
    "     \n",
    "    for term in TERMS:\n",
    "\n",
    "        params_lcl['text'] = term\n",
    "        offset = 3\n",
    "        #first_range = True\n",
    "\n",
    "        # initializing time index\n",
    "        params_lcl['min_upload_date'] = BEFORE_DATE\n",
    "    \n",
    "        ranges = ''\n",
    "        time_to_break = False\n",
    "\n",
    "        while True:\n",
    "            if VERBOSE >= 3: print(\"\".ljust(20, '+'))\n",
    "\n",
    "            params_lcl['max_upload_date'] = params_lcl['min_upload_date']\n",
    "            params_lcl['min_upload_date'] -= datetime.timedelta(days=offset)\n",
    "\n",
    "            next_batch_size = s.get(api_url, params=params_lcl).json()['photos']['total']\n",
    "            if VERBOSE >=3: print(f\"New date range: {params_lcl['min_upload_date']}\"\n",
    "                                 f\" to {params_lcl['max_upload_date']}______ \"\n",
    "                                  f\"total photos: {next_batch_size}\")\n",
    "\n",
    "            params_lcl['min_upload_date'], next_batch_size, offset =  find_best_date_range(\n",
    "                session=s, params=params_lcl,\n",
    "                start=params_lcl['min_upload_date'],\n",
    "                stop=params_lcl['max_upload_date'],\n",
    "                total_photos=next_batch_size,\n",
    "                offset=offset)\n",
    "\n",
    "            if (\n",
    "                params_lcl['min_upload_date'].timestamp() <= AFTER_DATE.timestamp()\n",
    "                #and int(next_batch_size) <= 4000\n",
    "            ):\n",
    "                time_to_break = True\n",
    "                params_lcl['min_upload_date'] = AFTER_DATE\n",
    "                next_batch_size = s.get(api_url, params=params_lcl).json()['photos']['total']\n",
    "                \n",
    "            if VERBOSE >= 2: print(f\"Next suitable range!: {params_lcl['min_upload_date']} \"\n",
    "                               f\"to {params_lcl['max_upload_date']}______ total \"\n",
    "                               f\"photos : {next_batch_size}\".ljust(120, ' '))\n",
    "\n",
    "            ranges += f\"\\n{term.replace(' ','_')},{params_lcl['min_upload_date']}\" \\\n",
    "                       f\",{params_lcl['max_upload_date']},{next_batch_size},Nay\"\n",
    "\n",
    "            if time_to_break: break\n",
    "        temp_file.write(ranges)\n",
    "\n",
    "        if VERBOSE >=3:\n",
    "            print(\"\".ljust(120, \"-\"))\n",
    "            print(f\"Finished for term: {term}   in date range : from {AFTER_DATE} to {BEFORE_DATE}\")\n",
    "            print(\"\".ljust(120, \"-\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def looping_over_date_ranges(path, params, session):\n",
    "    ''' loops over given date range'''\n",
    "\n",
    "    # copying request parameters to prepare to simultaneous execusion\n",
    "    params_lcl = deepcopy(params)\n",
    "    \n",
    "    last_response_time = 0\n",
    "    \n",
    "    TERMS = TAGS[-2:] if TEST and VERBOSE > 3 else TAGS\n",
    "     \n",
    "    for term in TERMS:\n",
    "        \n",
    "        csv_file.seek(0)\n",
    "\n",
    "        for date_range in ranges_reader:\n",
    "            if (\n",
    "                date_range['Search_Term'] == term.replace(' ', '_')\n",
    "                and date_range['Downloaded'] == 'Nay'\n",
    "            ):\n",
    "                if VERBOSE >= 3: print(date_range)\n",
    "                \n",
    "                try:\n",
    "                    date_format = '%Y-%m-%d %H:%M:%S.%f'\n",
    "                    start = date_range['Uploaded_After']\n",
    "                    stop = date_range['Uploaded_Before']\n",
    "                    params_lcl['min_upload_date'] = dt.strptime(start, date_format) \n",
    "                    params_lcl['max_upload_date'] = dt.strptime(stop, date_format) \n",
    "                except ValueError:\n",
    "                    date_format = '%Y-%m-%d %H:%M:%S'\n",
    "                    start = date_range['Uploaded_After']\n",
    "                    stop = date_range['Uploaded_Before']\n",
    "                    params_lcl['min_upload_date'] = dt.strptime(start, date_format) \n",
    "                    params_lcl['max_upload_date'] = dt.strptime(stop, date_format) \n",
    "                        \n",
    "                next_batch_size = date_range['Batch_Size']\n",
    "                if VERBOSE >= 3: print(\"\".ljust(20, '+'))\n",
    "                if VERBOSE >= 2: print(f\"Next range to download: {params_lcl['min_upload_date']}\\\n",
    "                to {params_lcl['max_upload_date']}______ total photos : {next_batch_size}\".ljust(120, ' '))\n",
    "                   \n",
    "                if VERBOSE >= 1: print(f'starting JSON dump...')\n",
    "                if TEST and VERBOSE > 3:\n",
    "                    print(\" fake writing to file \")\n",
    "                else:\n",
    "                    write_each_page_as_json_file(path=DATA_PATH, call_params=params_lcl, session=s, term=term)\n",
    "    \n",
    "                date_range['Downloaded'] = 'Aye'\n",
    "                ranges_writer.writerow(date_range)\n",
    "                time.sleep(0.2)\n",
    "                time.sleep(last_response_time * random.uniform(COURTESY_SLEEP[0], COURTESY_SLEEP[1]) * 2)\n",
    "                \n",
    "                \n",
    "        if VERBOSE >=3:\n",
    "            print(\"\".ljust(120, \"-\"))\n",
    "            print(f\"Finished for term: {term} in date range : from {AFTER_DATE} to {BEFORE_DATE}\")\n",
    "            print(\"\".ljust(120, \"-\"))\n",
    "\n",
    "        #time.sleep(random.uniform(COURTESY_SLEEP[0], COURTESY_SLEEP[1]) * 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def find_best_date_range(session, params, start, stop, total_photos, offset):\n",
    "\n",
    "    call_params = deepcopy(params)\n",
    "    # creating local variables to avoid multiprocessing issues down the line\n",
    "    call_params['per_page'] = 1\n",
    "    call_params['extras'] = ''\n",
    "\n",
    "    if VERBOSE >=3: print(f\"Finding a better range (in 20 attempts or less) ...\")\n",
    "    repeats = 0\n",
    "    nudge = 0\n",
    "\n",
    "    # This loop check whether the returned total photos are just a hair under\n",
    "    # which is the maximum allowed by the api. If not it cleverly adjusts the\n",
    "    # offset. The adjustment value used is difference ratio to the wanted\n",
    "    # number of photos (the assumption for this heuristic is that for a small\n",
    "    # range the uploaded photos density will not change much. And thus the\n",
    "    # difference % when applied to the range will give us a ballpark of the wanted range)\n",
    "\n",
    "    while (\n",
    "        int(total_photos) != 4000\n",
    "        and (\n",
    "            int(total_photos) > 4000\n",
    "            or int(total_photos) < ( 4000 * 0.99 )\n",
    "        )\n",
    "        and not (\n",
    "            int(total_photos) < 4000\n",
    "            and repeats > 20\n",
    "        )\n",
    "    ):\n",
    "\n",
    "        nudge = random.uniform(0, repeats / 100) \n",
    "        \n",
    "        if int(total_photos) > 4000:\n",
    "            if VERBOSE >= 3: print(f\"({str(repeats)}): too many    ({total_photos.ljust(5, '+')})\\\n",
    "            {start} --- {stop}\", end = '\\r')\n",
    "            # here the % will be small because we overshot the value wanted\n",
    "            try:\n",
    "                offset *= 4000/ int(total_photos)\n",
    "            except ZeroDivisionError:\n",
    "                offset *= 4000\n",
    "\n",
    "        if int(total_photos) < 4000:\n",
    "            if VERBOSE >= 3: print(f\"({str(repeats)}): not enough  ({total_photos.ljust(5, '-')})\\\n",
    "            {start} --- {stop}\", end = '\\r')\n",
    "            # here the % will be big because we underestimated the range\n",
    "            try:\n",
    "                offset *= (1 - nudge) * 4000/ int(total_photos)\n",
    "            except ZeroDivisionError:\n",
    "                offset *= 4000\n",
    "            \n",
    "        #if TEST and VERBOSE > 3: print(\"\\n\", offset, nudge)\n",
    "        start = stop - datetime.timedelta(offset)\n",
    "\n",
    "        # shifting the date range\n",
    "        call_params['min_upload_date'] = start\n",
    "        total_photos = session.get(api_url, params=call_params).json()['photos']['total']\n",
    "\n",
    "        repeats += 1\n",
    "        \n",
    "        #if VERBOSE >3 and TEST: time.sleep(0.3)\n",
    "    return start, total_photos, offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_each_page_as_json_file(path, call_params, session, term):\n",
    "\n",
    "    for each in added_params: call_params[each] = added_params[each]\n",
    "\n",
    "    # making_sure the request pages are correct\n",
    "    call_params['per_page'] = PHOTOS_PER_PAGE\n",
    "    pages = s.get(api_url, params=call_params).json()['photos']['max_allowed_pages']\n",
    "    \n",
    "    \n",
    "    start = str(call_params['min_upload_date'].timestamp())\n",
    "    stop = str(call_params['max_upload_date'].timestamp())\n",
    "    \n",
    "    for page in range(1, 1 + pages):\n",
    "        print(f\"Requesting page {page}...\".ljust(120, ' '),  end='\\r')\n",
    "\n",
    "        call_params['page'] = page\n",
    "\n",
    "        try:\n",
    "            before = dt.now().timestamp()\n",
    "            response = session.get(api_url, params=call_params)\n",
    "            after = dt.now().timestamp()\n",
    "            data = response.json()\n",
    "            data['api_call_params'] = call_params\n",
    "            data['api_call_params']['min_upload_date'] = start\n",
    "            data['api_call_params']['max_upload_date'] = stop\n",
    "        except Exception as e:\n",
    "            print(f\"Couldn't request JSON data:____ {e}\")\n",
    "\n",
    "        last_response_time = after - before\n",
    "\n",
    "        # Trying to write JSON data to file\n",
    "        file_to_be_written = f\"{path}{term.replace(' ','_')}_{start}-{stop}_{page}.json\"\n",
    "        #if VERBOSE >=3: print(f\" file path to be written: {file_to_be_written}\\n\\n\")\n",
    "        try:\n",
    "            with open(file_to_be_written, 'w') as outfile:\n",
    "                json.dump(data, outfile)\n",
    "\n",
    "            time_it_took = str(round(last_response_time, 2))\n",
    "            print(f\"{file_to_be_written} written succesfully! took {time_it_took} s\")\n",
    "            time.sleep(0.2)\n",
    "        except Exception as e:\n",
    "            print(f\"problem dumping json data: {str(e)}\")\n",
    "        if VERBOSE >=2: print(f'sleeping for {last_response_time * random.uniform(COURTESY_SLEEP[0], COURTESY_SLEEP[1])} seconds... (for courtesy :P )'.ljust(120, ' '), end = '\\r')\n",
    "        time.sleep(last_response_time * random.uniform(COURTESY_SLEEP[0], COURTESY_SLEEP[1]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title : \"Search: rain cloud | Flickr\"\n",
      "Getting AJAX data...\n",
      "attempting Scroll!\n",
      "Scrolled...\n",
      "ajax call intercepted!\n",
      "\n",
      "Extracted ajax params:--------\n",
      "\n",
      "api_key    :     2604abea1934cb357be10513eaec8614\n",
      "reqId      :     c84ce3ab\n",
      "api_url    :     https://api.flickr.com/services/rest\n",
      "extras     :     can_comment%2Ccount_comments%2Ccount_faves%2Cdescription%2Cisfavorite%2Clicense%2Cmedia%2Cneeds_interstitial%2Cowner_name%2Cpath_alias%2Crealname%2Crotation%2Curl_sq%2Curl_q%2Curl_t%2Curl_s%2Curl_n%2Curl_w%2Curl_m%2Curl_z%2Curl_c%2Curl_l\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Finished for term: rain cloud in date range : from 2019-01-06 13:40:00 to 2020-01-15 04:41:56.073299\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Finished for term: sun clouds in date range : from 2019-01-06 13:40:00 to 2020-01-15 04:41:56.073299\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Finished for term: sunny clouds in date range : from 2019-01-06 13:40:00 to 2020-01-15 04:41:56.073299\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Finished for term: clouds in date range : from 2019-01-06 13:40:00 to 2020-01-15 04:41:56.073299\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Finished for term: cloud in date range : from 2019-01-06 13:40:00 to 2020-01-15 04:41:56.073299\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Finished for term: sky in date range : from 2019-01-06 13:40:00 to 2020-01-15 04:41:56.073299\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Finished for term: storm in date range : from 2019-01-06 13:40:00 to 2020-01-15 04:41:56.073299\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "OrderedDict([('Search_Term', 'weather'), ('Uploaded_After', '2019-08-25 22:29:03.276049'), ('Uploaded_Before', '2020-01-15 01:33:10.762267'), ('Batch_Size', '3999'), ('Downloaded', 'Nay')])\n",
      "++++++++++++++++++++\n",
      "Next range to download: 2019-08-25 22:29:03.276049                to 2020-01-15 01:33:10.762267______ total photos : 3999\n",
      "starting JSON dump...\n",
      "./test/weather_1566786543.276049-1579069990.762267_1.json written succesfully! took 2.07 s                              \n",
      "./test/weather_1566786543.276049-1579069990.762267_2.json written succesfully! took 2.2 s                               \n",
      "./test/weather_1566786543.276049-1579069990.762267_3.json written succesfully! took 2.11 s                              \n",
      "./test/weather_1566786543.276049-1579069990.762267_4.json written succesfully! took 2.35 s                              \n",
      "./test/weather_1566786543.276049-1579069990.762267_5.json written succesfully! took 2.39 s                              \n",
      "./test/weather_1566786543.276049-1579069990.762267_6.json written succesfully! took 2.15 s                              \n",
      "./test/weather_1566786543.276049-1579069990.762267_7.json written succesfully! took 2.16 s                              \n",
      "./test/weather_1566786543.276049-1579069990.762267_8.json written succesfully! took 2.38 s                              \n",
      "./test/weather_1566786543.276049-1579069990.762267_9.json written succesfully! took 2.38 s                              \n",
      "./test/weather_1566786543.276049-1579069990.762267_10.json written succesfully! took 1.97 s                             \n",
      "./test/weather_1566786543.276049-1579069990.762267_11.json written succesfully! took 2.16 s                             \n",
      "./test/weather_1566786543.276049-1579069990.762267_12.json written succesfully! took 2.17 s                             \n",
      "./test/weather_1566786543.276049-1579069990.762267_13.json written succesfully! took 1.99 s                             \n",
      "./test/weather_1566786543.276049-1579069990.762267_14.json written succesfully! took 2.37 s                             \n",
      "./test/weather_1566786543.276049-1579069990.762267_15.json written succesfully! took 2.17 s                             \n",
      "./test/weather_1566786543.276049-1579069990.762267_16.json written succesfully! took 1.99 s                             \n",
      "sleeping for 4.782056000196399e-10 seconds... (for courtesy :P )                                                        \n",
      "OrderedDict([('Search_Term', 'weather'), ('Uploaded_After', '2019-04-17 15:14:49.011978'), ('Uploaded_Before', '2019-08-25 22:29:03.276049'), ('Batch_Size', '3993'), ('Downloaded', 'Nay')])\n",
      "++++++++++++++++++++\n",
      "Next range to download: 2019-04-17 15:14:49.011978                to 2019-08-25 22:29:03.276049______ total photos : 3993\n",
      "starting JSON dump...\n",
      "./test/weather_1555528489.011978-1566786543.276049_1.json written succesfully! took 15.32 s                             \n",
      "./test/weather_1555528489.011978-1566786543.276049_2.json written succesfully! took 5.83 s                              \n",
      "./test/weather_1555528489.011978-1566786543.276049_3.json written succesfully! took 5.65 s                              \n",
      "./test/weather_1555528489.011978-1566786543.276049_4.json written succesfully! took 7.11 s                              \n",
      "./test/weather_1555528489.011978-1566786543.276049_5.json written succesfully! took 7.46 s                              \n",
      "./test/weather_1555528489.011978-1566786543.276049_6.json written succesfully! took 7.09 s                              \n",
      "./test/weather_1555528489.011978-1566786543.276049_7.json written succesfully! took 7.28 s                              \n",
      "./test/weather_1555528489.011978-1566786543.276049_8.json written succesfully! took 6.07 s                              \n",
      "./test/weather_1555528489.011978-1566786543.276049_9.json written succesfully! took 7.09 s                              \n",
      "./test/weather_1555528489.011978-1566786543.276049_10.json written succesfully! took 5.64 s                             \n",
      "./test/weather_1555528489.011978-1566786543.276049_11.json written succesfully! took 7.53 s                             \n",
      "./test/weather_1555528489.011978-1566786543.276049_12.json written succesfully! took 5.65 s                             \n",
      "./test/weather_1555528489.011978-1566786543.276049_13.json written succesfully! took 6.68 s                             \n",
      "./test/weather_1555528489.011978-1566786543.276049_14.json written succesfully! took 6.62 s                             \n",
      "./test/weather_1555528489.011978-1566786543.276049_15.json written succesfully! took 6.11 s                             \n",
      "./test/weather_1555528489.011978-1566786543.276049_16.json written succesfully! took 2.18 s                             \n",
      "sleeping for 7.197118978104145e-10 seconds... (for courtesy :P )                                                        \n",
      "OrderedDict([('Search_Term', 'weather'), ('Uploaded_After', '2019-01-23 16:18:22.800346'), ('Uploaded_Before', '2019-04-17 15:14:49.011978'), ('Batch_Size', '3998'), ('Downloaded', 'Nay')])\n",
      "++++++++++++++++++++\n",
      "Next range to download: 2019-01-23 16:18:22.800346                to 2019-04-17 15:14:49.011978______ total photos : 3998\n",
      "starting JSON dump...\n",
      "./test/weather_1548278302.800346-1555528489.011978_1.json written succesfully! took 10.11 s                             \n",
      "./test/weather_1548278302.800346-1555528489.011978_2.json written succesfully! took 4.97 s                              \n",
      "./test/weather_1548278302.800346-1555528489.011978_3.json written succesfully! took 6.69 s                              \n",
      "./test/weather_1548278302.800346-1555528489.011978_4.json written succesfully! took 6.76 s                              \n",
      "./test/weather_1548278302.800346-1555528489.011978_5.json written succesfully! took 6.91 s                              \n",
      "./test/weather_1548278302.800346-1555528489.011978_6.json written succesfully! took 6.22 s                              \n",
      "./test/weather_1548278302.800346-1555528489.011978_7.json written succesfully! took 7.49 s                              \n",
      "./test/weather_1548278302.800346-1555528489.011978_8.json written succesfully! took 7.75 s                              \n",
      "./test/weather_1548278302.800346-1555528489.011978_9.json written succesfully! took 6.6 s                               \n",
      "./test/weather_1548278302.800346-1555528489.011978_10.json written succesfully! took 6.27 s                             \n",
      "./test/weather_1548278302.800346-1555528489.011978_11.json written succesfully! took 7.21 s                             \n",
      "./test/weather_1548278302.800346-1555528489.011978_12.json written succesfully! took 6.68 s                             \n",
      "./test/weather_1548278302.800346-1555528489.011978_13.json written succesfully! took 8.53 s                             \n",
      "./test/weather_1548278302.800346-1555528489.011978_14.json written succesfully! took 5.86 s                             \n",
      "./test/weather_1548278302.800346-1555528489.011978_15.json written succesfully! took 6.67 s                             \n",
      "./test/weather_1548278302.800346-1555528489.011978_16.json written succesfully! took 1.94 s                             \n",
      "sleeping for 1.822726954302071e-09 seconds... (for courtesy :P )                                                        \n",
      "OrderedDict([('Search_Term', 'weather'), ('Uploaded_After', '2019-01-06 13:40:00'), ('Uploaded_Before', '2019-01-23 16:18:22.800346'), ('Batch_Size', '3214'), ('Downloaded', 'Nay')])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "time data '2019-01-06 13:40:00' does not match format '%Y-%m-%d %H:%M:%S.%f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-197-adfcea0a5cf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfile_exists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mlooping_over_date_ranges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The dates ranges file for the period {AFTER_DATE} to {BEFORE_DATE} could not be found!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-190-8095f55cf4cd>\u001b[0m in \u001b[0;36mlooping_over_date_ranges\u001b[0;34m(path, params, session)\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Uploaded_After'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Uploaded_Before'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0mparams_lcl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'min_upload_date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0mparams_lcl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_upload_date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mnext_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Batch_Size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/_strptime.py\u001b[0m in \u001b[0;36m_strptime_datetime\u001b[0;34m(cls, data_string, format)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \"\"\"Return a class cls instance based on the input string and the\n\u001b[1;32m    576\u001b[0m     format string.\"\"\"\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff_fraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m     \u001b[0mtzname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         raise ValueError(\"time data %r does not match format %r\" %\n\u001b[0;32m--> 359\u001b[0;31m                          (data_string, format))\n\u001b[0m\u001b[1;32m    360\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         raise ValueError(\"unconverted data remains: %s\" %\n",
      "\u001b[0;31mValueError\u001b[0m: time data '2019-01-06 13:40:00' does not match format '%Y-%m-%d %H:%M:%S.%f'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    xhr_api_call, cookies = get_api_call_string()\n",
    "\n",
    "    parse_api_call(xhr_api_call)\n",
    "\n",
    "    extras = extras.replace('%2C', ',')\n",
    "\n",
    "    added_params = {\n",
    "        #\"extras\" : '',\n",
    "        \"extras\" : extras + ','+ ADD_EXTRAS,\n",
    "        \"per_page\" : 1,\n",
    "        \"api_key\" : api_key,\n",
    "        \"reqId\" : reqId,\n",
    "    }\n",
    "\n",
    "    for each in added_params: params[each] = added_params[each]\n",
    "\n",
    "    SPOOF_WEBDRIVER= False\n",
    "\n",
    "    if SPOOF_WEBDRIVER: ua = UserAgent()\n",
    "\n",
    "    with requests.sessions.Session() as s:\n",
    "        for cookie in cookies:\n",
    "            s.cookies.set(cookie['name'], cookie['value'])\n",
    "        if SPOOF_WEBDRIVER: s.headers['User-Agent'] = str(ua.chrome)\n",
    "\n",
    "    \n",
    "    date_format = '%Y_%m_%d'\n",
    "    \n",
    "    ranges_file = f'./date_ranges-{AFTER_DATE.strftime(date_format)}-{BEFORE_DATE.strftime(date_format)}.csv'\n",
    "    \n",
    "    temp_file = NamedTemporaryFile(mode='w', delete=False)\n",
    "    \n",
    "    csv_fields =['Search_Term','Uploaded_After','Uploaded_Before','Batch_Size','Downloaded']\n",
    "    \n",
    "    file_exists = True if os.path.isfile(ranges_file) else False\n",
    "    \n",
    "    #mode ='a' if file_exists else 'w'  \n",
    "        \n",
    "    with open(ranges_file,'r') as csv_file, temp_file:\n",
    "        ranges_reader = csv.DictReader(csv_file, fieldnames=csv_fields)\n",
    "        ranges_writer = csv.DictWriter(temp_file, fieldnames=csv_fields)\n",
    "        temp_file.write('Search_Term,Uploaded_After,Uploaded_Before,Batch_Size,Downloaded\\n')\n",
    "        \n",
    "        if GET_DATES_ONLY:\n",
    "            construct_date_ranges(path=DATA_PATH, params=params, session=s)\n",
    "        else:\n",
    "            if file_exists:\n",
    "                looping_over_date_ranges(path=DATA_PATH, params=params, session=s)   \n",
    "            else:\n",
    "                print(f\"The dates ranges file for the period {AFTER_DATE} to {BEFORE_DATE} could not be found!\")\n",
    "            \n",
    "        # Writing changes to the original file\n",
    "    shutil.move(temp_file.name, ranges_file)   \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-01-15 04:51:53.641778'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = '2019-08-25 22:29:03.276049'\n",
    "dt.now().strftime('%Y-%m-%d %H:%M:%S.%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 8, 25, 22, 29, 3, 276049)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.strptime(date,'%Y-%m-%d %H:%M:%S.%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
